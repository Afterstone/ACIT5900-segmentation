{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import open_clip\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook:\n",
    "    \"\"\"Attaches to a module and records its activations and gradients.\"\"\"\n",
    "\n",
    "    def __init__(self, module: nn.Module):\n",
    "        self.data = None\n",
    "        self.hook = module.register_forward_hook(self.save_grad)\n",
    "        \n",
    "    def save_grad(self, module, input, output):\n",
    "        self.data = output\n",
    "        output.requires_grad_(True)\n",
    "        output.retain_grad()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        self.hook.remove()\n",
    "        \n",
    "    @property\n",
    "    def activation(self) -> torch.Tensor:\n",
    "        return self.data\n",
    "    \n",
    "    @property\n",
    "    def gradient(self) -> torch.Tensor:\n",
    "        return self.data.grad\n",
    "\n",
    "\n",
    "# Reference: https://arxiv.org/abs/1610.02391\n",
    "def gradCAM(\n",
    "    model: nn.Module,\n",
    "    input: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    layer: nn.Module\n",
    ") -> torch.Tensor:\n",
    "    # Zero out any gradients at the input.\n",
    "    if input.grad is not None:\n",
    "        input.grad.data.zero_()\n",
    "        \n",
    "    # Disable gradient settings.\n",
    "    requires_grad = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        requires_grad[name] = param.requires_grad\n",
    "        param.requires_grad_(False)\n",
    "        \n",
    "    # Attach a hook to the model at the desired layer.\n",
    "    assert isinstance(layer, nn.Module)\n",
    "    with Hook(layer) as hook:        \n",
    "        # Do a forward and backward pass.\n",
    "        output = model(input)\n",
    "        output.backward(target)\n",
    "\n",
    "        grad = hook.gradient.float()\n",
    "        act = hook.activation.float()\n",
    "    \n",
    "        # Global average pool gradient across spatial dimension\n",
    "        # to obtain importance weights.\n",
    "        alpha = grad.mean(dim=(2, 3), keepdim=True)\n",
    "        # Weighted combination of activation maps over channel\n",
    "        # dimension.\n",
    "        gradcam = torch.sum(act * alpha, dim=1, keepdim=True)\n",
    "        # We only want neurons with positive influence so we\n",
    "        # clamp any negative ones.\n",
    "        gradcam = torch.clamp(gradcam, min=0)\n",
    "\n",
    "    # Resize gradcam to input resolution.\n",
    "    gradcam = F.interpolate(\n",
    "        gradcam,\n",
    "        input.shape[2:],\n",
    "        mode='bicubic',\n",
    "        align_corners=False)\n",
    "    \n",
    "    # Restore gradient settings.\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad_(requires_grad[name])\n",
    "        \n",
    "    return gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x: np.ndarray) -> np.ndarray:\n",
    "    # Normalize to [0, 1].\n",
    "    x = x - x.min()\n",
    "    if x.max() > 0:\n",
    "        x = x / x.max()\n",
    "    return x\n",
    "\n",
    "# Modified from: https://github.com/salesforce/ALBEF/blob/main/visualization.ipynb\n",
    "def getAttMap(img, attn_map, blur=True):\n",
    "    if blur:\n",
    "        attn_map = gaussian_filter(attn_map, 0.02*max(img.shape[:2]))\n",
    "    attn_map = normalize(attn_map)\n",
    "    cmap = plt.get_cmap('jet')\n",
    "    attn_map_c = np.delete(cmap(attn_map), 3, 2)\n",
    "    attn_map = 1*(1-attn_map**0.7).reshape(attn_map.shape + (1,))*img + \\\n",
    "            (attn_map**0.7).reshape(attn_map.shape+(1,)) * attn_map_c\n",
    "    return attn_map\n",
    "\n",
    "def viz_attn(img, attn_map, blur=True):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(img)\n",
    "    # Rescale the attention map to match the image size.\n",
    "    attn_map = np.array(Image.fromarray(attn_map).resize((img.shape[1], img.shape[0])))\n",
    "    axes[1].imshow(getAttMap(img, attn_map, blur))\n",
    "    for ax in axes:\n",
    "        ax.axis(\"off\")\n",
    "    return fig, axes\n",
    "    \n",
    "def load_image(img_path, resize=None):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    if resize is not None:\n",
    "        image = image.resize((resize, resize))\n",
    "    return np.asarray(image).astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_clip import pretrained\n",
    "\n",
    "for model_name, weights in pretrained.list_pretrained():\n",
    "    if 'convnext' not in model_name.lower():\n",
    "        continue\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    print(f\"Weights: {weights}\")\n",
    "    print()\n",
    "\n",
    "# Find vit types.\n",
    "print(set(\"-\".join(m.split('-')[:2]) for m,w in pretrained.list_pretrained() if 'vit' in m.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('convnext_xxlarge', pretrained='laion2b_s34b_b82k_augreg')\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-g-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_url = 'https://images2.minutemediacdn.com/image/upload/c_crop,h_706,w_1256,x_0,y_64/f_auto,q_auto,w_1100/v1554995050/shape/mentalfloss/516438-istock-637689912.jpg'\n",
    "image_url = \"https://static.toiimg.com/photo/79693966.cms\"\n",
    "image_path = 'image.png'\n",
    "urllib.request.urlretrieve(image_url, image_path)\n",
    "\n",
    "texts = [\n",
    "    # \"the pommes frites\", \n",
    "    'the hamburger', \n",
    "    # 'hamburger', \n",
    "    # 'the pizza',\n",
    "    # 'the lettuce',\n",
    "    # 'tomato', \n",
    "    # 'the tomato', \n",
    "    # 'hamburger bun', \n",
    "    # 'cheese',\n",
    "    # \"food\",\n",
    "    # \"cutting board\",\n",
    "    # \"the meat patty\",\n",
    "    # \"ground beef\",\n",
    "]\n",
    "tokenized_text = tokenizer(texts)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    text_features = model.encode_text(tokenized_text.cuda())\n",
    "\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0).cuda()\n",
    "image_np = load_image(image_path, resize=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as TVT\n",
    "from tqdm import trange\n",
    "\n",
    "augment = TVT.Compose([TVT.ColorJitter(0.5, 0.5, 0.5, 0.5)])\n",
    "augment = torch.nn.Identity()\n",
    "\n",
    "attn_maps_3 = []\n",
    "attn_maps_4 = []\n",
    "with torch.cuda.amp.autocast():\n",
    "    for n in trange(10):\n",
    "        img = augment(image)\n",
    "        attn_map = gradCAM(model.visual, image, text_features, model.visual.trunk.stages[2])\n",
    "        attn_maps_3.append(attn_map.squeeze().detach().cpu().numpy())\n",
    "        attn_map = gradCAM(model.visual, image, text_features, model.visual.trunk.stages[3])\n",
    "        attn_maps_4.append(attn_map.squeeze().detach().cpu().numpy())\n",
    "np_att_3 = np.stack(attn_maps_3, axis=0)\n",
    "np_att_4 = np.stack(attn_maps_4, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_map = np.mean(np_att_3, axis=0)\n",
    "# fig, axes = viz_attn(image_np, pred_map, blur=False)\n",
    "\n",
    "pred_map = np.mean(np.power(np_att_4, 2), axis=0)\n",
    "pred_map = pred_map / pred_map.max()\n",
    "pred_map = pred_map * (pred_map > 0.5)\n",
    "fig, axes = viz_attn(image_np, pred_map, blur=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
